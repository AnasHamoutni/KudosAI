{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Play Tic-Tac-Toe Against a Q-Learning Agent**\n",
        "\n",
        "Hello,\n",
        "\n",
        "My name is **Anas HAMOUTNI** and I am a **Moroccan Data Scientist**.\n",
        "\n",
        "**Tic Tac Toe** is a classic two-player game where one player uses '**X**' and the other uses '**O**'. The goal is to **get three of your own markers in a row**, either horizontally, vertically, or diagonally. The game ends with a winner if someone achieves this or a draw if the board is filled without any three-in-a-row.\n",
        "\n",
        "This project implements a **console-based Tic Tac Toe game** where a human plays against an AI that learns through **[Q-learning](https://www.simplilearn.com/tutorials/machine-learning-tutorial/what-is-q-learning)**. The game includes features like **random starting turns**, **intelligent AI moves**, **scoring**, and **replay options**. All the rules of Tic Tac Toe are properly enforced, including **legal moves** and **checking for the winner**.\n",
        "\n",
        "### **Rules**\n",
        "\n",
        "The game follows the standard rules of **Tic Tac Toe**:\n",
        "\n",
        "*   The game is played on a 3x3 grid.\n",
        "*   The player is '**-1**', and the AI is '**1**'.\n",
        "*   Players take turns to place their marks in empty squares.\n",
        "*   The first player to get three of their marks in a horizontal, vertical, or diagonal row wins.\n",
        "\n",
        "If all nine squares are filled without a winner, the game is a draw.\n",
        "\n",
        "### **About Q-learning:**\n",
        "\n",
        "**Q-learning** is a model-free **reinforcement learning algorithm** used to find the optimal action-selection policy for a given finite **[Markov decision process](hhttps://www.youtube.com/watch?v=2iF9PRriA7w)**.\n",
        "\n",
        "The algorithm works by learning a **Q-function**, represented by $Q(s, a)$, where $s$ is the current state and $a$ is the chosen action.\n",
        "\n",
        "The **Q-function** represents the expected return (**cumulative reward**) when taking action $a$ in state $s$ and following the optimal policy thereafter.\n",
        "\n",
        "The **Q-values** are iteratively updated using the following formula:\n",
        "$$\n",
        "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)$$\n",
        "\n",
        "Here, $α$ is the learning rate, $r$ is the immediate reward received after taking action $a$, $γ$ is the discount factor that determines the importance of future rewards, $s'$ is the next state, and $a'$ is the next action.\n",
        "\n",
        "The term $\\max_{a'} Q(s', a')$ represents the **maximum Q-value** for the next state, which essentially captures the expected return of the best action in the next state.\n",
        "\n",
        "**The algorithm iterates through states and actions**, continually updating **Q-values** until convergence, to ultimately obtain the optimal policy.\n",
        "\n",
        "--------------\n",
        "\n",
        "**Enjoy playing against the AI, and may the best player win!**"
      ],
      "metadata": {
        "id": "DtD7iFaFSrRR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "MeRlyL4PSpfv"
      },
      "outputs": [],
      "source": [
        "#@title ##**1. Import Libraries** { display-mode: \"form\" }\n",
        "\n",
        "# Importing essential libraries for numerical operations and deep learning\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**2. Initialize Q-table** { display-mode: \"form\" }\n",
        "\n",
        "# Initialize Q-table\n",
        "Q = {}\n",
        "alpha = 0.5  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.1  # Exploration rate\n"
      ],
      "metadata": {
        "id": "d_AmZRVCY3zC"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**3. Helper Functions** { display-mode: \"form\" }\n",
        "\n",
        "# Function to convert the board to a hashable string (flattening it into a one-dimensional array and then converting it to a string)\n",
        "def board_to_key(board):\n",
        "    return str(board.reshape(-1))\n",
        "\n",
        "# Function to find available actions (i.e., empty cells) from the current state of the board\n",
        "def available_actions(board):\n",
        "    return np.argwhere(board == 0) # Returns the indices of the cells where the value is 0 (empty)\n",
        "\n",
        "# Function to choose an action and return its index using a policy that combines exploration and exploitation\n",
        "def choose_action(board):\n",
        "    key = board_to_key(board) # Convert the board to a string key\n",
        "    if key not in Q:\n",
        "        Q[key] = np.zeros(len(available_actions(board))) # Initialize Q-values if key not present\n",
        "    actions = available_actions(board) # Get available actions\n",
        "    if random.random() < epsilon: # With probability epsilon, choose a random action (exploration)\n",
        "        action_index = random.randint(0, len(actions) - 1)\n",
        "    else:\n",
        "        action_index = np.argmax(Q[key]) # Otherwise, choose the action with the highest Q-value (exploitation)\n",
        "    return actions[action_index], action_index # Return the chosen action and its index\n",
        "\n",
        "# Function to update the Q-table using the Q-learning update rule\n",
        "def update_q_table(prev_state, action_index, reward, current_state):\n",
        "    prev_key = board_to_key(prev_state) # Convert previous state to string key\n",
        "    current_key = board_to_key(current_state) # Convert current state to string key\n",
        "\n",
        "    if prev_key not in Q: # If previous key not present, initialize Q-values\n",
        "        Q[prev_key] = np.zeros(len(available_actions(prev_state)))\n",
        "\n",
        "    if current_key not in Q: # If current key not present, initialize Q-values\n",
        "        Q[current_key] = np.zeros(len(available_actions(current_state)))\n",
        "\n",
        "    max_q_value_next_state = np.max(Q[current_key]) if Q[current_key].size != 0 else 0 # Get the maximum Q-value for the next state\n",
        "\n",
        "    # Apply the Q-learning update formula\n",
        "    Q[prev_key][action_index] += alpha * (reward + gamma * max_q_value_next_state - Q[prev_key][action_index])\n",
        "\n",
        "\n",
        "# Function to check if there's a winner (3 in a row horizontally, vertically, or diagonally)\n",
        "def check_winner(board):\n",
        "    for row in range(3):\n",
        "        if board[row, 0] == board[row, 1] == board[row, 2] != 0:\n",
        "            return True # Check horizontal\n",
        "    for col in range(3):\n",
        "        if board[0, col] == board[1, col] == board[2, col] != 0:\n",
        "            return True # Check vertical\n",
        "    if board[0, 0] == board[1, 1] == board[2, 2] != 0:\n",
        "        return True # Check main diagonal\n",
        "    if board[0, 2] == board[1, 1] == board[2, 0] != 0:\n",
        "        return True # Check other diagonal\n",
        "    return False # No winner\n",
        "\n",
        "# Function to check if the board is full (i.e., a draw)\n",
        "def is_draw(board):\n",
        "    return not (board == 0).any() # Check if there are any zeros (empty cells) in the board\n",
        "\n",
        "# Function for handling the human player's move, including input validation\n",
        "def human_move(board):\n",
        "    while True:\n",
        "        move = input(\"Enter your move (0-8): \") # Get input from the user\n",
        "        try:\n",
        "            move = int(move) # Convert input to integer\n",
        "            if move < 0 or move > 8:\n",
        "                raise ValueError(\"Move out of range\") # Check if input is within the valid range\n",
        "            row, col = divmod(move, 3) # Convert the input to row and column indices\n",
        "            if board[row, col] == 0:\n",
        "                return row, col # Return the chosen row and column if the cell is empty\n",
        "            else:\n",
        "                print(\"Illegal move. Try again.\") # If cell is not empty, prompt to try again\n",
        "        except ValueError: # Handle non-integer or out-of-range inputs\n",
        "            print(\"Invalid input. Please enter an integer between 0 and 8.\")"
      ],
      "metadata": {
        "id": "BIhMV2P-ZGrc"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**4.  The Game Loop** { display-mode: \"form\" }\n",
        "\n",
        "# Function to play Tic Tac Toe game between a user and an AI\n",
        "def play():\n",
        "    user_score = 0 # Initialize user's score\n",
        "    ai_score = 0 # Initialize AI's score\n",
        "\n",
        "    # Main game loop\n",
        "    while True:\n",
        "        board = np.zeros((3, 3)) # Initialize the game board\n",
        "        turn = 'user' if random.random() < 0.5 else 'ai' # Randomly decide who goes first\n",
        "        print(f\"{turn.capitalize()} goes first!\") # Print who is going first\n",
        "        prev_state = None # Store previous state of the board\n",
        "        action_index = None # Store the index of the action taken\n",
        "\n",
        "        # Loop for each turn in the game\n",
        "        while True:\n",
        "            print(board) # Print the current state of the board\n",
        "\n",
        "            # If it's the user's turn\n",
        "            if turn == 'user':\n",
        "                row, col = human_move(board) # Get the user's move\n",
        "                board[row, col] = 1 # Update the board with the user's move\n",
        "                if check_winner(board): # Check if the user has won\n",
        "                    print(board) # Print the updated board\n",
        "                    print(\"You win!\") # Declare user's victory\n",
        "                    user_score += 1 # Increment user's score\n",
        "                    break # Exit the turn loop\n",
        "                elif is_draw(board): # Check for a draw\n",
        "                    print(board)\n",
        "                    print(\"It's a draw!\")\n",
        "                    if prev_state is not None:\n",
        "                        update_q_table(prev_state, action_index, 0, board) # Update Q-table for a draw\n",
        "                    break\n",
        "                turn = 'ai' # Change the turn to the AI\n",
        "\n",
        "            # If it's the AI's turn\n",
        "            else:\n",
        "                action, action_index = choose_action(board) # Choose the AI's action\n",
        "                row, col = tuple(action) # Extract row and column from the action\n",
        "                board[row, col] = -1 # Update the board with the AI's move\n",
        "                if check_winner(board): # Check if the AI has won\n",
        "                    print(board)  # Print the updated board\n",
        "                    print(\"AI wins!\") # Declare AI's victory\n",
        "                    ai_score += 1 # Increment AI's score\n",
        "                    if prev_state is not None:\n",
        "                        update_q_table(prev_state, action_index, 1, board) # Update Q-table with win\n",
        "                    break\n",
        "                elif is_draw(board): # Check for a draw\n",
        "                    print(\"It's a draw!\")\n",
        "                    if prev_state is not None:\n",
        "                        update_q_table(prev_state, action_index, 0, board) # Update Q-table for a draw\n",
        "                    break\n",
        "                if prev_state is not None:\n",
        "                    update_q_table(prev_state, action_index, 0, board) # Update Q-table for regular move\n",
        "                prev_state = board.copy() # Save the current state as the previous state for the next iteration\n",
        "                turn = 'user' # Change the turn to the user\n",
        "        print(f\"Score: User {user_score} - AI {ai_score}\") # Print the current score\n",
        "        replay = input(\"Play again? (y/n): \") # Ask if the player wants to play again\n",
        "        if replay.lower() != 'y':\n",
        "            break # Exit the main game loop if the answer is not 'y'"
      ],
      "metadata": {
        "id": "V-A1616bZZlc"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**5. Start the Game** { display-mode: \"form\" }\n",
        "\n",
        "play() # Start the game"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56B4jrl4ZmZ7",
        "outputId": "ad9d35a1-f245-4f20-c624-3d1907580b65"
      },
      "execution_count": 66,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User goes first!\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "Enter your move (0-8): 1\n",
            "[[0. 1. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "[[-1.  1.  0.]\n",
            " [ 0.  0.  0.]\n",
            " [ 0.  0.  0.]]\n",
            "Enter your move (0-8): 4\n",
            "[[-1.  1.  0.]\n",
            " [ 0.  1.  0.]\n",
            " [ 0.  0.  0.]]\n",
            "[[-1.  1. -1.]\n",
            " [ 0.  1.  0.]\n",
            " [ 0.  0.  0.]]\n",
            "Enter your move (0-8): 7\n",
            "[[-1.  1. -1.]\n",
            " [ 0.  1.  0.]\n",
            " [ 0.  1.  0.]]\n",
            "You win!\n",
            "Score: User 1 - AI 0\n",
            "Play again? (y/n): N\n"
          ]
        }
      ]
    }
  ]
}